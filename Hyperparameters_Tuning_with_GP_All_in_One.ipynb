{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48EA0EkszD73"
      },
      "source": [
        "**Hyperparameters Tuning Code for RL-ANN (Delta Loss) with GBM Path Simulator Using GP (Bayesian Optimization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmABweu-z0Yg",
        "outputId": "d58e74cd-d57d-42ac-e294-7467492cf87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GpyOpt\n",
            "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20 kB 33.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GpyOpt) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.7/dist-packages (from GpyOpt) (1.4.1)\n",
            "Collecting GPy>=1.8\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[K     |████████████████████████████████| 959 kB 28.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GpyOpt) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GpyOpt) (0.29.26)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy>=1.8->GpyOpt) (4.4.2)\n",
            "Building wheels for collected packages: GpyOpt, GPy, paramz\n",
            "  Building wheel for GpyOpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GpyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83609 sha256=84f093ce35b7036bf41626bb97372d34775764967ed1418961694041865e62fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/fa/d1/f9652b5af79f769a0ab74dbead7c7aea9a93c6bc74543fd3ec\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp37-cp37m-linux_x86_64.whl size=2565120 sha256=f7858890101255c191812b152f2b061b3bb3c15a3d6389c886d51702b9fa5c40\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/18/28/dd1ce0192a81b71a3b086fd952511d088b21e8359ea496860a\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102566 sha256=75a7d03d5c5ad5967b2225b4137a428a649c9b32d5fd41a9c2c7be1928329b36\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/95/f5/ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
            "Successfully built GpyOpt GPy paramz\n",
            "Installing collected packages: paramz, GPy, GpyOpt\n",
            "Successfully installed GPy-1.10.0 GpyOpt-1.2.6 paramz-0.9.5\n"
          ]
        }
      ],
      "source": [
        "!pip install GpyOpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9MMh9tj0MAG",
        "outputId": "66d8d1de-3813-4610-cee1-c4f44f7084a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r67hoHD30QwY",
        "outputId": "42bd96be-0506-40a5-b59b-f461dabca6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Research Projects/Year 2 Summer Research/RL-Option Pricing/RL_ANN\n"
          ]
        }
      ],
      "source": [
        "cd RL_ANN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RSJxg5kGik2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "8019c2bb-e4a8-42f9-8cee-84808f355356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TRAINING\n",
            "Hyperparameters_Set: Number of Hidden Units: 64 64 Activation Function: 2 Learning_Scheduler: 1 Number of Batches: 1000 Batch Size: 15000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-12ade90909c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mmyBopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"mod_file_RL_ANN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mmyBopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPyOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mmyBopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"bayesian_optimizer_RL_ANN_log.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"bayes_opt_RL_ANN_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmyBopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_evaluations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"ev_file_RL_ANN\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#if not saved, then create an empty file in result_new_train called \"ev_file_RL_ANN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minitial_design_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_design_chooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m_init_design_chooser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Case 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mrlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-12ade90909c8>\u001b[0m in \u001b[0;36mmyfunc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# myfunc() means give me a x (combination) and I will give you back the outcome of that x and save it in a csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmyfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mhidden_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_RL_ANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Hyperparameters_Tuning/GBM_Delta/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-12ade90909c8>\u001b[0m in \u001b[0;36mexecute_RL_ANN\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get batch of theoretical options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                     \u001b[0mtraining_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "matplotlib.use('Agg')\n",
        "np.random.seed(496)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(496)\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "directory = '.'\n",
        "import os\n",
        "import pickle\n",
        "import csv\n",
        "import os.path\n",
        "import GPyOpt\n",
        "import math\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "GP=True # Gaussian Processes\n",
        "out_dir = os.path.split(os.path.realpath('__file__'))[0]+'/Hyperparameters_Tuning/GBM_Delta/'\n",
        "\n",
        "# execute_RL-ANN = calling the hyperparameter tunning training function\n",
        "#x= [neurons in hidden layer 1, neurons in hiddden layer 2, activation_function, learning_rate_scheduler, # of batches, batch_size]\n",
        "def execute_RL_ANN(x):\n",
        "    # network for synthetic data\n",
        "    hidden_layers = [int(x[:, 0]), int(x[:, 1])]  # hn1, hn2 (50, 1000, 10)\n",
        "    activation_x = x[:, 2]\n",
        "    if activation_x == 0:\n",
        "      activation = tf.tanh\n",
        "    elif activation_x == 1:\n",
        "      activation = tf.nn.relu\n",
        "    else:\n",
        "      activation = tf.sigmoid\n",
        "    n_outputs = 1\n",
        "    learning_scheduler = x[:, 3]\n",
        "    if learning_scheduler == 0:\n",
        "      learning_rate = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate = 5e-4, first_decay_steps = 100, t_mul=2.0, m_mul=1.0, alpha=0.0,name=None)\n",
        "    elif learning_scheduler == 1:\n",
        "      learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 5e-4, decay_steps = 100000, decay_rate=0.96, staircase=False, name=None)\n",
        "    else:\n",
        "      learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries = [50,100,150], values = [5e-4, 1e-4, 5e-5, 1e-5], name=None)\n",
        "    ann = tf.keras.Sequential(\n",
        "                layers=\n",
        "                [tf.keras.layers.Dense(hidden_layers[0], activation = activation, input_shape=(2,))] + \\\n",
        "                [tf.keras.layers.Dense(hidden_layers[i], activation = activation) for i in range(1, len(hidden_layers))] + \\\n",
        "                [tf.keras.layers.Dense(n_outputs, activation = tf.keras.activations.softplus)],\n",
        "                name=\"ann\")\n",
        "    # define optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)  \n",
        "    # define loss function\n",
        "    hedging_mse = tf.keras.losses.MeanSquaredError()\n",
        "    #input processing\n",
        "    def process_input(X_input, X_input_):\n",
        "            r = tf.fill([tf.shape(input=X_input)[0],1], 0., name = 'r') # interest rate, if applicable\n",
        "            S = tf.cast(tf.slice(X_input, (0,0), (-1,1)), tf.float32)\n",
        "            K = tf.cast(tf.slice(X_input, (0,1), (-1,1)), tf.float32)\n",
        "            T = tf.cast(tf.slice(X_input, (0,2), (-1,1)), tf.float32)\n",
        "            S_ = tf.cast(tf.slice(X_input_, (0,0), (-1,1)), tf.float32)\n",
        "            T_ = tf.cast(tf.slice(X_input_, (0,1), (-1,1)), tf.float32)\n",
        "            return S, K, T, S_, T_, r\n",
        "\n",
        "    # Simulating geometric Brownian motion -- the stock simulator\n",
        "    def stock_sim_path(S, alpha, delta, sigma, T, N, n):\n",
        "        \"\"\"Simulates geometric Brownian motion.\"\"\"\n",
        "        h = T/n\n",
        "        mean = (alpha - delta - .5*sigma**2)*h\n",
        "        vol = sigma * h**.5\n",
        "        return S*np.exp((mean + vol*np.random.randn(n,N)).cumsum(axis = 0))\n",
        "\n",
        "    def get_batch2(stock_path,n, moneyness_range = (.5,2)): \n",
        "        \"\"\"Constructs theoretical options based on the time series stock_path\"\"\"\n",
        "        picks = np.random.randint(0, len(stock_path)-1, n.astype(np.int64))\n",
        "        T = np.random.randint(1, 500, (n.astype(np.int64)[0],1))\n",
        "        S = stock_path[picks]\n",
        "        S_ = stock_path[picks+1]\n",
        "        K = np.random.uniform(*moneyness_range, (n.astype(np.int64)[0],1))*S\n",
        "        X = np.hstack([S, K, T/250])\n",
        "        X_ = np.hstack([S_, (T-1)/250])\n",
        "        return X, X_\n",
        "\n",
        "    @tf.function\n",
        "    def loss(X_input, X_input_):\n",
        "      ''' Loss Function with only Delta term expanded using delta-hedging principle'''\n",
        "      S, K, T, S_, T_, r = process_input(X_input, X_input_)\n",
        "      with tf.GradientTape() as tape2:\n",
        "            tape2.watch(S)\n",
        "            X = tf.concat([S/(K*tf.exp(-r*T)), T], 1) #input matrix for ANN\n",
        "            X_ = tf.concat([S_/(K*tf.exp(-r*T_)), T_], 1) #input matrix for ANN_\n",
        "\n",
        "            out = ann(X)\n",
        "            out = K*tf.where(tf.greater(T, 1e-3), out, tf.maximum(S/K - 1, 0))\n",
        "            out_ = ann(X_)\n",
        "            out_ = K*tf.where(tf.greater(T_, 1e-3), out_, tf.maximum(S_/K - 1, 0))\n",
        "      delta = tape2.gradient(out, S)\n",
        "      delta = tf.maximum(delta,0)\n",
        "      delta = tf.minimum(delta, 1)\n",
        "      return hedging_mse(delta*(S_-S), out_-out)\n",
        "\n",
        "    @tf.function\n",
        "    def grad(X_train, X_train_):\n",
        "      with tf.GradientTape() as tape:\n",
        "        tape.watch(ann.trainable_variables)\n",
        "        loss_value = loss(X_train, X_train_)\n",
        "      return loss_value, tape.gradient(loss_value, ann.trainable_variables)\n",
        "\n",
        "    @tf.function\n",
        "    def training_op(X_train, X_train_):\n",
        "        loss_value, grads = grad(X_train, X_train_)\n",
        "        optimizer.apply_gradients(zip(grads, ann.trainable_variables)) \n",
        "\n",
        "    #model training\n",
        "    n_epochs = 200 #number of training epochs\n",
        "    n_batches = x[:,4]\n",
        "    batch_size= x[:,5]\n",
        "    T=2\n",
        "    days = int(250*T)\n",
        "    stock_path = stock_sim_path(100, .06386, 0, .07425, T, 1, days) #simulate stock path\n",
        "    stock_path_test = stock_sim_path(100, .06386, 0, .07425, T, 1, days) #simulate stock path for cross-validation\n",
        "    losses = []\n",
        "    X_test, X_test_ = get_batch2(stock_path_test, batch_size) #get test-set\n",
        "    print(\"START TRAINING\")\n",
        "    print(\"Hyperparameters_Set:\", \"Number of Hidden Units:\", hidden_layers[0], hidden_layers[1], \"Activation Function:\", int(activation_x), \"Learning_Scheduler:\", int(learning_scheduler),\n",
        "          \"Number of Batches:\", int(n_batches), \"Batch Size:\", int(batch_size))\n",
        "    for epoch in range(int(n_epochs)):\n",
        "            for batch in range(int(n_batches)):\n",
        "                    X_train, X_train_ = get_batch2(stock_path, batch_size) # get batch of theoretical options\n",
        "                    training_op(X_train, X_train_)\n",
        "            epoch_loss = loss(X_test, X_test_)\n",
        "            losses.append(epoch_loss)\n",
        "            print('Epoch:', epoch, 'Loss:', epoch_loss.numpy())\n",
        "    plt.figure()\n",
        "    plt.semilogy(np.arange(n_epochs), losses)\n",
        "    plt.xlim([0, n_epochs])\n",
        "    return hidden_layers, activation_x, learning_scheduler, n_batches, batch_size, losses\n",
        "\n",
        "# myfunc() means give me a x (combination) and I will give you back the outcome of that x and save it in a csv file\n",
        "def myfunc(x):\n",
        "    hidden_layers, activation, learning_rate, n_batches, batch_size, losses = execute_RL_ANN(x)\n",
        "    out_dir = os.path.split(os.path.realpath('__file__'))[0]+'/Hyperparameters_Tuning/GBM_Delta/'\n",
        "    avg_loss = np.mean(losses[-51:-1])\n",
        "    myCsvRow = hidden_layers + [activation] + [learning_rate] + [n_batches] + [batch_size]\n",
        "    print(\"New row : \", myCsvRow)\n",
        "    with open(out_dir+\"Hyperparameters_tuning_results_RL_ANN.csv\", \"a\") as file:\n",
        "        writer = csv.writer(file, delimiter=\";\")\n",
        "        writer.writerow(myCsvRow)\n",
        "    return float(avg_loss)\n",
        "\n",
        "# bounds define the range of hyperparameters from where the agent will sample from == our hypercube\n",
        "bounds =[{'name': 'nh', 'type': 'discrete', 'domain': (32,64,128),'dimensionality': 1},\n",
        "         {'name': 'nh2', 'type': 'discrete', 'domain': (32,64,128),'dimensionality': 1},\n",
        "         {'name': 'activation', 'type': 'discrete', 'domain': (0,1,2),'dimensionality': 1},\n",
        "         {'name': 'learning_scheduler', 'type': 'discrete', 'domain': (0,1,2),'dimensionality': 1},\n",
        "         {'name': 'nbatches', 'type': 'discrete', 'domain': (1000,1500,2000),'dimensionality': 1},\n",
        "         {'name': 'batch_size', 'type': 'discrete', 'domain': (10000,15000,20000),'dimensionality': 1}]\n",
        "\n",
        "for i in range(103): # we will run 100 experiments of GP => 100 befief updates of the good region.\n",
        "    existing_bayesian_optimizer = os.path.isfile(out_dir + \"ev_file_RL_ANN\") # ev_file is a simple txt file (empty @ beginning) \"warning \\n\"\n",
        "    if GP:\n",
        "        if existing_bayesian_optimizer :\n",
        "            evals = pd.read_csv(out_dir+\"ev_file_RL_ANN\", index_col=0, delimiter=\"\\t\")\n",
        "            Y = np.array([[x] for x in evals[\"Y\"]],dtype=np.float32)   # Just want add dimensions ??\n",
        "            X = np.array(evals.filter(regex=\"var*\"),dtype=np.float32)\n",
        "            myBopt = GPyOpt.methods.BayesianOptimization(f=myfunc, domain=bounds, constraints=None, Y=Y,X=X, initial_design_numdata=3, maximize=False)\n",
        "            myBopt.run_optimization(max_iter=1, verbosity=True, report_file=out_dir+\"bayesian_optimizer_RL_ANN_log.txt\", models_file=out_dir+\"bayes_opt_RL_ANN_model\", eps=0)\n",
        "            myBopt.save_evaluations(out_dir+\"ev_file_RL_ANN\")\n",
        "            myBopt.save_models(out_dir+\"mod_file_RL_ANN\")\n",
        "        else :\n",
        "            myBopt = GPyOpt.methods.BayesianOptimization(f=myfunc, domain=bounds, constraints=None, initial_design_numdata=3, maximize=False)\n",
        "            myBopt.run_optimization(max_iter=1, verbosity=True, report_file=out_dir+\"bayesian_optimizer_RL_ANN_log.txt\", models_file=out_dir+\"bayes_opt_RL_ANN_model\", eps=0)\n",
        "            myBopt.save_evaluations(out_dir+\"ev_file_RL_ANN\") #if not saved, then create an empty file in result_new_train called \"ev_file_RL_ANN\"\n",
        "            myBopt.save_models(out_dir+\"mod_file_RL_ANN\")\n",
        "    else:\n",
        "        pass\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}